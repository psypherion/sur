# rec-engine/utils.py

import os
import json
import torch
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
import joblib
from typing import Dict, Any, List, Tuple, Optional

# Import local modules
from model import GatedMultimodalRecommender
# Import config from preprocessor to ensure consistency
from preprocessor import CONFIG as CONFIG

# --- Configuration ---
ARTIFACTS_DIR = "processed_data"
PROCESSED_DATA_PATH = os.path.join(ARTIFACTS_DIR, "preprocessed_song_data.json")
MODEL_PATH = "gated_recommender_model.pth"
SCALAR_SCALER_PATH = os.path.join(ARTIFACTS_DIR, "scalar_scaler.pkl")
CHORD_MAP_PATH = os.path.join(ARTIFACTS_DIR, "chord_map.json")
N_CLUSTERS = 20

# Initialize NLP models as global variables to be loaded on demand
embedding_model = None
sentiment_pipeline = None

def initialize_text_models_if_needed():
    """A helper to initialize NLP models only once, when first needed."""
    global embedding_model, sentiment_pipeline
    if embedding_model is None:
        from sentence_transformers import SentenceTransformer
        from transformers import pipeline
        print("Initializing text processing models...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {device.upper()}")
        embedding_model = SentenceTransformer(CONFIG["embedding_model"], device=device)
        sentiment_pipeline = pipeline("sentiment-analysis", model=CONFIG["sentiment_model"], tokenizer=CONFIG["sentiment_model"], device=0 if device == "cuda" else -1)
        print("Text models initialized.")

@torch.no_grad()
def load_all_artifacts() -> Tuple[pd.DataFrame, GatedMultimodalRecommender, MinMaxScaler, Dict, KMeans]:
    """
    Loads all necessary artifacts and prepares the main DataFrame by pre-computing
    embeddings and audio clusters for the entire local song library.
    """
    print("Loading all necessary artifacts...")
    
    try:
        df = pd.read_json(PROCESSED_DATA_PATH)
        scalar_scaler = joblib.load(SCALAR_SCALER_PATH)
        with open(CHORD_MAP_PATH, 'r', encoding='utf-8') as f:
            chord_map = json.load(f)
    except FileNotFoundError as e:
        print(f"[FATAL ERROR] An essential artifact is missing: {e.filename}. Please run preprocessor.py.")
        exit()

    audio_dim = len(df.iloc[0]['audio_features_vector'])
    text_dim = len(df.iloc[0]['text_features_vector'])
    model = GatedMultimodalRecommender(audio_input_dim=audio_dim, text_input_dim=text_dim, embedding_dim=256)
    
    try:
        model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))
    except FileNotFoundError:
        print(f"[FATAL ERROR] Trained model not found at '{MODEL_PATH}'. Please run train.py.")
        exit()
    model.eval()
    print("✅ Artifacts loaded successfully.")

    print("Pre-computing final gated embeddings for all local songs...")
    audio_tensors = torch.tensor(np.array(df['audio_features_vector'].tolist()), dtype=torch.float32)
    text_tensors = torch.tensor(np.array(df['text_features_vector'].tolist()), dtype=torch.float32)
    all_embeddings, all_alphas = model(audio_tensors, text_tensors)
    df['final_embedding'] = list(all_embeddings.numpy())
    df['alpha'] = list(all_alphas.numpy().flatten())
    print("✅ Gated embeddings computed.")

    print(f"Performing offline audio clustering into {N_CLUSTERS} neighborhoods...")
    audio_vectors_np = np.array(df['audio_features_vector'].tolist())
    audio_vectors_float32 = audio_vectors_np.astype(np.float32)
    kmeans_model = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init='auto').fit(audio_vectors_float32)
    df['audio_cluster'] = kmeans_model.labels_
    print("✅ Audio clustering complete.")
    
    popularity_scaler = MinMaxScaler()
    df['popularity_score'] = popularity_scaler.fit_transform(df[['popularity']])
    print("✅ Popularity scores normalized.")
    
    return df, model, scalar_scaler, chord_map, kmeans_model

# --- THIS IS THE FULLY CORRECTED FUNCTION DEFINITION ---
def vectorize_dynamic_song(
    song_data: Dict, 
    scalar_scaler: MinMaxScaler, 
    chord_map: Dict,
    embedding_model_instance, # Argument for the embedding model
    sentiment_pipeline_instance # Argument for the sentiment pipeline
) -> Optional[Tuple[torch.Tensor, torch.Tensor]]:
    """
    Uses preprocessor logic to vectorize song data generated by Gemini.
    This version correctly accepts the NLP models as arguments.
    """
    print("   > Vectorizing dynamically generated data...")
    try:
        df = pd.DataFrame([song_data])
        
        scalar_features = CONFIG['scalar_audio_features']
        for feature in scalar_features:
            if feature not in df.columns: df[feature] = 0.5
        df[scalar_features] = scalar_scaler.transform(df[scalar_features])
        
        harmonic_vector = np.zeros(len(chord_map), dtype=int)
        prog = df.iloc[0].get('chord_progression', {})
        if isinstance(prog, dict):
            for chord in str(prog.get('main', '')).split(' - '):
                if chord in chord_map: harmonic_vector[chord_map[chord]] = 1
        
        mfcc_vector = [0.0] * CONFIG['n_mfcc']
        audio_vector_list = df[scalar_features].iloc[0].tolist() + harmonic_vector.tolist() + mfcc_vector
        
        lyrics_content = df.iloc[0].get('lyrics', "")
        if isinstance(lyrics_content, dict):
            lyrics_text = " ".join(str(v) for v in lyrics_content.values())
        elif isinstance(lyrics_content, str):
            lyrics_text = lyrics_content
        else:
            lyrics_text = ""

        if not lyrics_text.strip():
            embedding_dim = embedding_model_instance.get_sentence_embedding_dimension()
            semantic_embedding = [0.0] * embedding_dim
            sentiment_vector = [0.0, 1.0, 0.0]
        else:
            embedding = embedding_model_instance.encode([lyrics_text])
            semantic_embedding = embedding[0].tolist()
            sentiment_results = sentiment_pipeline_instance(lyrics_text, truncation=True)
            scores = {'positive': 0.0, 'neutral': 0.0, 'negative': 0.0}
            result = sentiment_results[0]
            label = result['label'].lower()
            scores[label] = result['score']
            sentiment_vector = [scores['negative'], scores['neutral'], scores['positive']]

        text_vector_list = semantic_embedding + sentiment_vector
        
        return torch.tensor([audio_vector_list], dtype=torch.float32), torch.tensor([text_vector_list], dtype=torch.float32)
    except Exception as e:
        print(f"   [ERROR] Failed to vectorize dynamic data: {e}")
        import traceback
        traceback.print_exc()
        return None